# -*- coding: utf-8 -*-

"""
@Time    : 2025/12/24 20:59
@File    : stage1_extract_meta.py
@Author  : zj
@Description:

Stage 1: æ·±åº¦è§£ææ¯ç¯‡åšå®¢ï¼Œç”Ÿæˆç»“æ„åŒ–å…ƒæ•°æ®ï¼ˆä¿å­˜è‡³ .tmp/ï¼‰
âœ… æ”¯æŒç¼“å­˜è·³è¿‡
âœ… è®°å½•æ¯ç¯‡è€—æ—¶ & tokens
âœ… å…¨æ–‡å‚ä¸ LLM åˆ†æï¼ˆåˆ†å—åæ±‡æ€»ï¼‰
âœ… å¥å£®çš„ API è°ƒç”¨ä¸ JSON è¾“å‡ºï¼ˆé˜² YAML å†’å·é—®é¢˜ï¼‰
âœ… è‡ªåŠ¨ä¿®å¤è½»å¾® JSON æˆªæ–­
âœ… LLM è‡ªæˆ‘æ ¡éªŒï¼ˆself-refineï¼‰ç¡®ä¿æ ¼å¼è§„èŒƒï¼ˆä»…æ ¼å¼ï¼Œä¸æ”¹å†…å®¹ï¼‰
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
import frontmatter
import openai
import time
import sys
import json

# === é…ç½® ===
BLOG_POSTS_DIR = Path("blog/source/_posts")
TMP_OUTPUT_DIR = Path(".tmp")
TMP_OUTPUT_DIR.mkdir(exist_ok=True)

MODEL_NAME = "deepseek-chat"
openai.base_url = "https://api.deepseek.com/v1/"
openai.api_key = os.getenv("OPENAI_API_KEY", "sk-")

MAX_RETRIES = 3
CHUNK_SIZE = 8000
OVERLAP = 500

global_stats = {
    'total_articles': 0,
    'processed_articles': 0,
    'skipped_articles': 0,
    'total_duration_sec': 0.0,
    'total_prompt_tokens': 0,
    'total_completion_tokens': 0,
    'total_tokens': 0,
}


def clean_markdown(md: str) -> str:
    md = re.sub(r"```.*?```", "", md, flags=re.DOTALL)
    md = re.sub(r"`[^`]*`", "", md)
    md = re.sub(r"!\[[^\]]*\]\([^)]*\)", "", md)
    md = re.sub(r"\[([^\]]+)\]\([^)]*\)", r"\1", md)
    md = re.sub(r"^#+\s*", "", md, flags=re.MULTILINE)
    md = re.sub(r"^>.*$", "", md, flags=re.MULTILINE)
    ref_patterns = [
        r"(?i)##?\s*(å‚è€ƒ|ç›¸å…³é˜…è¯»|å»¶ä¼¸é˜…è¯»|æ¨èé˜…è¯»|æ›´å¤šèµ„æ–™|å‚è€ƒèµ„æ–™|reference|further reading|è‡´è°¢|é¸£è°¢)"
    ]
    for pat in ref_patterns:
        parts = re.split(pat, md, maxsplit=1)
        md = parts[0]
    return md.strip()


def split_text_into_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:
    if len(text) <= chunk_size:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
    return chunks


def call_llm(prompt: str, temperature: float = 0.2, max_tokens: int = 4096) -> Tuple[str, Dict[str, int], float]:
    start_time = time.time()
    try:
        response = openai.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=90
        )
        duration = time.time() - start_time
        content = response.choices[0].message.content.strip()
        usage = getattr(response, 'usage', None)
        usage_dict = {
            'prompt_tokens': getattr(usage, 'prompt_tokens', 0),
            'completion_tokens': getattr(usage, 'completion_tokens', 0),
            'total_tokens': getattr(usage, 'total_tokens', 0)
        }

        global_stats['total_prompt_tokens'] += usage_dict['prompt_tokens']
        global_stats['total_completion_tokens'] += usage_dict['completion_tokens']
        global_stats['total_tokens'] += usage_dict['total_tokens']

        return content, usage_dict, duration

    except Exception as e:
        duration = time.time() - start_time
        raise RuntimeError(f"API è°ƒç”¨å¤±è´¥ (è€—æ—¶ {duration:.2f}s): {e}")


def robust_llm_call(prompt: str, max_retries=MAX_RETRIES, temperature=0.2, max_tokens=4096) -> Tuple[str, Dict, float]:
    for i in range(1, max_retries + 1):
        try:
            content, usage, duration = call_llm(prompt, temperature=temperature, max_tokens=max_tokens)
            out = content.strip()

            # ç§»é™¤å¯èƒ½çš„ markdown åŒ…è£¹
            if out.startswith("```json"):
                out = out[7:]
            elif out.startswith("```"):
                out = out[3:]
            if out.endswith("```"):
                out = out[:-3]
            out = out.strip()

            return out, usage, duration
        except Exception as e:
            wait = min(2 ** i, 10)
            print(f"  âš ï¸  Retry {i}/{max_retries}: {e}, wait {wait}s", file=sys.stderr)
            time.sleep(wait)
    raise RuntimeError("LLM call failed after retries")


def refine_meta_output(draft_json_str: str) -> str:
    """
    ä½¿ç”¨ LLM ä»…ä¿®å¤ JSON æ ¼å¼é—®é¢˜ï¼Œä¸¥ç¦ä¿®æ”¹ä»»ä½•è¯­ä¹‰å†…å®¹ã€‚
    """
    refine_prompt = f"""ä½ æ˜¯ä¸€ä½ JSON æ ¼å¼æ ¡éªŒå™¨ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™å¤„ç†è¾“å…¥ï¼š

ã€ä»»åŠ¡ã€‘
- å¦‚æœè¾“å…¥å·²æ˜¯åˆæ³• JSONï¼Œè¯·åŸæ ·è¿”å›ï¼ˆä¸è¦æ”¹åŠ¨ä»»ä½•ä¸€ä¸ªå­—ï¼ŒåŒ…æ‹¬æ ‡ç‚¹ã€ç©ºæ ¼ã€æªè¾ï¼‰ã€‚
- å¦‚æœè¾“å…¥å­˜åœ¨æ ¼å¼é—®é¢˜ï¼ˆå¦‚ç¼ºå°‘åŒå¼•å·ã€ç¼ºå°‘é€—å·ã€æœªé—­åˆæ‹¬å·ã€åŒ…å« Markdown ä»£ç å—ç­‰ï¼‰ï¼Œè¯·ä»…ä¿®å¤æ ¼å¼ï¼Œä½¿å…¶æˆä¸ºåˆæ³• JSONã€‚
- **ç»å¯¹ç¦æ­¢ä¿®æ”¹ main_ideaã€tags çš„å®é™…æ–‡å­—å†…å®¹**ï¼Œå³ä½¿å†…å®¹æœ‰äº‹å®é”™è¯¯ã€é€»è¾‘ä¸ç¬¦æˆ–ä¸ç¬¦åˆè§„èŒƒï¼Œä¹Ÿå¿…é¡»åŸæ ·ä¿ç•™ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- ä»…è¾“å‡ºä¿®å¤åçš„ JSON å­—ç¬¦ä¸²ï¼›
- ä¸è¦ä»»ä½•è§£é‡Šã€æ³¨é‡Šã€å‰ç¼€ã€åç¼€ï¼›
- å¿…é¡»ä½¿ç”¨åŒå¼•å·ï¼›
- ç¡®ä¿å¯è¢« Python json.loads() è§£æã€‚

ã€å¾…å¤„ç†å†…å®¹ã€‘
{draft_json_str}

ç°åœ¨è¯·è¾“å‡ºä¿®å¤åçš„ JSONï¼š"""

    try:
        refined, _, _ = robust_llm_call(refine_prompt, temperature=0.0, max_tokens=1024)
        return refined
    except Exception as e:
        print(f"  âš ï¸ æ ¡éªŒå¤±è´¥ï¼Œå›é€€åŸå§‹è¾“å‡º: {e}", file=sys.stderr)
        return draft_json_str


def build_single_chunk_prompt(title: str, content: str, existing_cats, existing_tags) -> str:
    ex_cat_str = ", ".join(flatten_list(existing_cats)) if existing_cats else "æ— "
    ex_tag_str = ", ".join(flatten_list(existing_tags)) if existing_tags else "æ— "
    # æ›¿æ¢åŸæ¥çš„ build_single_chunk_prompt ä¸­çš„ JSON æ ¼å¼è¯´æ˜
    return f"""ä½ æ˜¯ä¸€ä½èµ„æ·±æŠ€æœ¯åšå®¢ç¼–è¾‘ï¼Œè¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡ç« ï¼Œè¾“å‡ºå…¶ç»“æ„åŒ–å…ƒä¿¡æ¯ã€‚

    è¦æ±‚ï¼š
    1. **æ–‡ç« æ•´ä½“æ€æƒ³**ï¼ˆmain_ideaï¼‰ï¼š
    - ç”¨ 1~2 å¥è¯æ¦‚æ‹¬ï¼ˆæ€»å­—æ•° â‰¤ 200 å­—ï¼‰ï¼›
    - å¿…é¡»ä½“ç°æ–‡ç« æ€§è´¨ï¼Œä¾‹å¦‚ï¼š
        - â€œæœ¬æ–‡æ˜¯å¯¹ã€ŠDVC: An End-to-end...ã€‹çš„è¯¦ç»†è§£è¯»ã€‚â€
        - â€œè¿™æ˜¯ä¸€ä»½ OpenDVC å¼€æºé¡¹ç›®çš„å®ç°æŠ¥å‘Šã€‚â€
        - â€œæœ¬æ–‡æ€»ç»“äº†ä½œè€… 2024 å¹´åœ¨ AI å·¥ç¨‹åŒ–æ–¹å‘çš„å­¦ä¹ ä¸é¡¹ç›®å®è·µã€‚â€
    - ç¦æ­¢ä½¿ç”¨è‹±æ–‡å•å¼•å· '...'ï¼Œè®ºæ–‡æ ‡é¢˜è¯·ç”¨ä¸­æ–‡ä¹¦åå·ã€Š...ã€‹
    2. **æ ‡ç­¾åˆ—è¡¨**ï¼ˆtagsï¼Œæœ€å¤š 20 ä¸ªï¼‰ï¼š
    - æ ¼å¼ï¼š`ä¸­æ–‡/English`ï¼ˆå¦‚ï¼šå…‰æµ/Optical Flowï¼‰ï¼›
    - çº¯è‹±æ–‡æœ¯è¯­å¯ç›´æ¥å†™ï¼ˆå¦‚ï¼šYOLOv8ï¼‰ï¼›
    - å¿…é¡»æ˜¯æ–‡ç« ä¸»åŠ¨è®²è§£æˆ–ä½¿ç”¨çš„æŠ€æœ¯å®ä½“ã€‚

    å·²æœ‰ front-matterï¼ˆä»…ä½œå‚è€ƒï¼‰ï¼š
    - åˆ†ç±»: {ex_cat_str}
    - æ ‡ç­¾: {ex_tag_str}

    æ–‡ç« æ ‡é¢˜: {title}
    æ–‡ç« æ­£æ–‡:
    {content[:12000]}

    è¯·**ä»…è¾“å‡ºæ ‡å‡† JSON**ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    {{"main_idea": "...", "tags": [...]}}

    âš ï¸ é‡è¦ï¼š
    - ä¸è¦ä»»ä½•è§£é‡Šã€æ³¨é‡Šã€markdown ä»£ç å—ï¼›
    - å­—ç¬¦ä¸²å¿…é¡»ç”¨åŒå¼•å·ï¼›
    - ä¸è¦åŒ…å«ä»»ä½•è‹±æ–‡å•å¼•å· '...'ï¼›
    - è®ºæ–‡æ ‡é¢˜è¯·ç”¨ã€Š...ã€‹ã€‚
    """


def build_multi_chunk_final_prompt(title: str, combined_summary: str, existing_cats, existing_tags) -> str:
    ex_cat_str = ", ".join(flatten_list(existing_cats)) if existing_cats else "æ— "
    ex_tag_str = ", ".join(flatten_list(existing_tags)) if existing_tags else "æ— "
    return f"""åŸºäºä»¥ä¸‹æ‘˜è¦ç”Ÿæˆæœ€ç»ˆå…ƒä¿¡æ¯ã€‚

è¦æ±‚ï¼š
- main_ideaï¼š1~2 å¥ï¼ˆâ‰¤200 å­—ï¼‰ï¼Œä½“ç°æ–‡ç« æ€§è´¨ï¼Œç”¨ã€Šã€‹æ ‡æ³¨è®ºæ–‡åï¼›
- tagsï¼šæœ€å¤š 20 ä¸ªï¼Œæ ¼å¼ `ä¸­æ–‡/English`ã€‚

å·²æœ‰ front-matter:
- åˆ†ç±»: {ex_cat_str}
- æ ‡ç­¾: {ex_tag_str}

æ–‡ç« æ ‡é¢˜: {title}
æ–‡ç« æŠ€æœ¯æ‘˜è¦:
{combined_summary}

è¯·**ä»…è¾“å‡ºæ ‡å‡† JSON**ï¼š
{{"main_idea": "...", "tags": [...]}}

âš ï¸ ä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ï¼"""


def flatten_list(lst):
    result = []
    for item in lst:
        if isinstance(item, list):
            result.extend(flatten_list(item))
        else:
            result.append(str(item).strip())
    return result


def try_fix_json(json_str: str) -> str:
    """å°è¯•ä¿®å¤å¸¸è§çš„ JSON æˆªæ–­é—®é¢˜"""
    s = json_str.strip()
    if not s.endswith('}'):
        if '"tags": [' in s and not s.rstrip().endswith(']'):
            s = s.rstrip() + "]}"
        elif s.count('{') > s.count('}'):
            s = s.rstrip() + "}"
    return s


def extract_meta_from_article(file_path: Path) -> Tuple[Dict[str, Any], float, Dict]:
    start_time = time.time()
    out_file = TMP_OUTPUT_DIR / (file_path.stem + ".json")

    if out_file.exists():
        with open(out_file, 'r', encoding='utf-8') as f:
            try:
                cached = json.load(f)
                if cached and isinstance(cached, dict) and 'main_idea' in cached:
                    duration = time.time() - start_time
                    print(f"  ğŸ’¾ å‘½ä¸­ç¼“å­˜ï¼Œè·³è¿‡è§£æ", file=sys.stderr)
                    return cached, duration, {}
            except Exception as e:
                print(f"  âš ï¸ ç¼“å­˜æŸåï¼Œé‡æ–°è§£æ: {e}", file=sys.stderr)

    post = frontmatter.load(str(file_path))
    title = post.get("title", file_path.stem)
    content = post.content
    existing_cats = post.get("categories", [])
    existing_tags = post.get("tags", [])

    clean_content = clean_markdown(content)
    chunks = split_text_into_chunks(clean_content)

    raw_output = ""
    total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
    total_duration = 0.0

    if len(chunks) == 1:
        full_prompt = build_single_chunk_prompt(title, chunks[0], existing_cats, existing_tags)
        raw_output, usage, duration = robust_llm_call(full_prompt, max_tokens=1024)
        total_usage = usage
        total_duration = duration
    else:
        summaries = []
        for i, chunk in enumerate(chunks):
            prompt = f"""ä½ æ˜¯æŠ€æœ¯æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·æ€»ç»“ä»¥ä¸‹æ–‡ç« ç‰‡æ®µçš„æ ¸å¿ƒæŠ€æœ¯å†…å®¹ï¼ˆå¿½ç•¥ç¤ºä¾‹ã€å¼•ç”¨ã€é“¾æ¥ï¼‰ï¼š
ç‰‡æ®µ {i + 1}/{len(chunks)}:
{chunk[:4000]}
---
ä»…è¾“å‡ºä¸€æ®µç®€æ´çš„æŠ€æœ¯æ‘˜è¦ï¼ˆ50å­—å†…ï¼‰ï¼Œä¸è¦ç¼–å·ã€‚"""
            summary, usage, duration = robust_llm_call(prompt, temperature=0.1, max_tokens=256)
            summaries.append(summary)
            for k in total_usage:
                total_usage[k] += usage.get(k, 0)
            total_duration += duration

        combined_summary = " ".join(summaries)
        full_prompt = build_multi_chunk_final_prompt(title, combined_summary, existing_cats, existing_tags)
        raw_output, usage, duration = robust_llm_call(full_prompt, max_tokens=1024)
        for k in total_usage:
            total_usage[k] += usage.get(k, 0)
        total_duration += duration

    # === Step 1: è‡ªæˆ‘æ ¡éªŒï¼ˆä»…æ ¼å¼ï¼‰===
    refined_output = refine_meta_output(raw_output)

    # === æ–°å¢ï¼šå¦‚æœ refine è¾“å‡ºä¸ºç©ºæˆ–æ˜æ˜¾æ— æ•ˆï¼Œå›é€€åˆ° raw_output ===
    def is_plausibly_json(s: str) -> bool:
        s = s.strip()
        return s.startswith('{') and s.endswith('}')

    final_output_to_parse = refined_output
    if not is_plausibly_json(refined_output):
        print("  âš ï¸ refined è¾“å‡ºæ— æ•ˆï¼Œå°è¯•ä½¿ç”¨åŸå§‹è¾“å‡º", file=sys.stderr)
        final_output_to_parse = raw_output

    # === Step 2: å°è¯•è§£æ ===
    data = None
    last_error = None
    for candidate in [final_output_to_parse, refined_output, raw_output]:
        try:
            fixed = try_fix_json(candidate.strip())
            data = json.loads(fixed)
            break
        except json.JSONDecodeError as e:
            last_error = e
            continue

    if data is None:
        print(f"  âŒ æ‰€æœ‰å°è¯•å‡å¤±è´¥: {last_error}", file=sys.stderr)
        preview_raw = raw_output[:500].replace('\n', '\\n')
        preview_refined = refined_output[:500].replace('\n', '\\n')
        print(f"  ğŸ” raw å‰ 500 å­—ç¬¦: {preview_raw}", file=sys.stderr)
        print(f"  ğŸ” refined å‰ 500 å­—ç¬¦: '{preview_refined}'", file=sys.stderr)

        # ä¿å­˜è°ƒè¯•æ–‡ä»¶
        (TMP_OUTPUT_DIR / (file_path.stem + ".raw")).write_text(raw_output, encoding="utf-8")
        (TMP_OUTPUT_DIR / (file_path.stem + ".refined")).write_text(refined_output, encoding="utf-8")

        return {
            "error": f"Failed to parse JSON after all attempts. Last error: {str(last_error)}"}, time.time() - start_time, {}

    if not isinstance(data, dict):
        raise ValueError("LLM è¿”å›éå­—å…¸ç»“æ„")

    # æ¸…æ´— tags
    tags = data.get("tags", [])
    cleaned_tags = []
    for tag in tags:
        if not isinstance(tag, str):
            continue
        tag = tag.strip()
        if not tag or re.fullmatch(r"\d{4}", tag):
            continue
        if "/" in tag:
            parts = [p.strip() for p in tag.split("/", 1)]
            if len(parts) == 2:
                zh_part, en_part = parts
                if zh_part.lower() == en_part.lower():
                    cleaned_tags.append(en_part)
                else:
                    cleaned_tags.append(f"{zh_part}/{en_part}")
            else:
                cleaned_tags.append(tag)
        else:
            cleaned_tags.append(tag)

    seen = set()
    deduped = []
    for t in cleaned_tags:
        if t not in seen:
            deduped.append(t)
            seen.add(t)

    data["tags"] = deduped[:20]
    data["source_file"] = file_path.name
    return data, time.time() - start_time, total_usage


def main():
    md_files = list(BLOG_POSTS_DIR.rglob("*.md"))
    global_stats['total_articles'] = len(md_files)
    print(f"ğŸ” æ‰¾åˆ° {len(md_files)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹æ·±åº¦è§£æ...", file=sys.stderr)

    for i, fp in enumerate(md_files, 1):
        out_file = TMP_OUTPUT_DIR / (fp.stem + ".json")  # â† æ”¹ä¸º .json
        if out_file.exists():
            global_stats['skipped_articles'] += 1
            print(f"[{i}/{len(md_files)}] â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {fp.name}")
            continue

        print(f"[{i}/{len(md_files)}] ğŸ§  è§£æ: {fp.name}")
        try:
            meta, duration, usage = extract_meta_from_article(fp)
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)  # â† ä¿å­˜ä¸ºæ ‡å‡† JSON
            global_stats['processed_articles'] += 1
            global_stats['total_duration_sec'] += duration

            pt = usage.get('prompt_tokens', 0)
            ct = usage.get('completion_tokens', 0)
            tt = usage.get('total_tokens', 0)
            print(f"  âœ… è€—æ—¶: {duration:.2f}s | Tokens: {pt}/{ct} â†’ {tt} | ä¿å­˜è‡³ {out_file.name}")
        except Exception as e:
            print(f"  âŒ è·³è¿‡ {fp.name}: {e}", file=sys.stderr)

    print("\n" + "=" * 60, file=sys.stderr)
    print(f"ğŸ‰ Stage 1 å®Œæˆï¼", file=sys.stderr)
    print(f"ğŸ“Š æ€»è®¡: {global_stats['total_articles']} ç¯‡", file=sys.stderr)
    print(f"   âœ… å¤„ç†: {global_stats['processed_articles']}", file=sys.stderr)
    print(f"   â­ï¸  è·³è¿‡: {global_stats['skipped_articles']}", file=sys.stderr)
    print(f"   â±ï¸  æ€»è€—æ—¶: {global_stats['total_duration_sec']:.2f} ç§’", file=sys.stderr)
    print(
        f"   ğŸ”¢ æ€» Tokens: {global_stats['total_prompt_tokens']} + {global_stats['total_completion_tokens']} = {global_stats['total_tokens']}",
        file=sys.stderr)
    print("=" * 60, file=sys.stderr)


if __name__ == "__main__":
    main()
