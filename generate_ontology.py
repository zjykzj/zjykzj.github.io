# -*- coding: utf-8 -*-

"""
@Time    : 2025/12/14
@File    : generate_ontology.py
@Author  : zj
@Description:

ç¬¬ä¸€é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆåšå®¢çš„æ ‡å‡†åŒ–åˆ†ç±»ä½“ç³»ï¼ˆcategories.yamlï¼‰ä¸æ ‡ç­¾è¯è¡¨ï¼ˆtag_vocabulary.yamlï¼‰

åŠŸèƒ½è¯´æ˜ï¼š
- åŸºäºç°æœ‰åšå®¢æ–‡ç« çš„å†…å®¹ã€æ ‡é¢˜åŠå·²æœ‰ front-matter ä¿¡æ¯ï¼Œ
  åˆ©ç”¨å¤§æ¨¡å‹ï¼ˆDeepSeekï¼‰è‡ªåŠ¨å½’çº³å‡ºä¸€å¥—**ç»“æ„åŒ–ã€å¯ç»´æŠ¤ã€è¯­ä¹‰ä¸€è‡´**çš„é¢„å®šä¹‰åˆ†ç±»ä¸æ ‡ç­¾ä½“ç³»ï¼›
- **åˆ†ç±»ï¼ˆCategoriesï¼‰èšç„¦æ–‡ç« ç±»å‹/ç›®çš„**ï¼ˆå¦‚â€œæ•™ç¨‹ > å·¥å…·ä½¿ç”¨â€ã€â€œå¤ç›˜ > é¡¹ç›®æ€»ç»“â€ï¼‰ï¼Œè€ŒéæŠ€æœ¯ä¸»é¢˜ï¼›
- **æ ‡ç­¾ï¼ˆTagsï¼‰èšç„¦å…·ä½“æŠ€æœ¯å®ä½“**ï¼ˆå¦‚ Dockerã€YOLOv5ã€CI/CDï¼‰ï¼Œå¹¶è‡ªåŠ¨åˆå¹¶åˆ«åï¼ˆaliasesï¼‰ï¼›
- æ”¯æŒå¤§è§„æ¨¡æ–‡ç« å¤„ç†ï¼šæ ‡ç­¾ç”Ÿæˆé‡‡ç”¨åˆ†æ‰¹ç­–ç•¥ï¼ˆé»˜è®¤æ¯æ‰¹ 50 ç¯‡ï¼‰ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼›
- å†…ç½®å¼ºå¥å®¹é”™æœºåˆ¶ï¼š
    â€¢ LLM è°ƒç”¨è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š 3~5 æ¬¡ï¼‰
    â€¢ YAML è¾“å‡ºè§£æå¤±è´¥æ—¶è‡ªåŠ¨çº é”™é‡è¯•
    â€¢ å¼‚å¸¸æ‰¹æ¬¡ä¿å­˜åŸå§‹è°ƒè¯•æ–‡ä»¶ï¼ˆ.rawï¼‰
- å…¨ç¨‹è®°å½• token æ¶ˆè€—ã€è¯·æ±‚æ¬¡æ•°ä¸è€—æ—¶ï¼Œä¾¿äºæˆæœ¬è¯„ä¼°ã€‚

è¾“å…¥ï¼š
  - åšå®¢æ–‡ç« ç›®å½•ï¼ˆé»˜è®¤: blog/source/_postsï¼‰
  - æ¯ç¯‡æ–‡ç« éœ€ä¸º Markdown æ ¼å¼ï¼Œå¯å«ç°æœ‰ categories/tagsï¼ˆç”¨äºå‚è€ƒï¼‰

è¾“å‡ºï¼š
  - categories.yamlï¼šæ ‡å‡†åŒ–åˆ†ç±»ä½“ç³»ï¼ˆå« path / description / matching_hintsï¼‰
  - tag_vocabulary.yamlï¼šæ ‡å‡†åŒ–æ ‡ç­¾è¯è¡¨ï¼ˆå« standard_tag / aliasesï¼‰
  - ï¼ˆå¯é€‰ï¼‰å¤±è´¥æ‰¹æ¬¡çš„ .raw è°ƒè¯•æ–‡ä»¶

å…¸å‹ä½¿ç”¨æ–¹å¼ï¼š

# ç›´æ¥è¿è¡Œï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
python generate_ontology.py

# æ³¨æ„ï¼š
# - æœ¬è„šæœ¬é€šå¸¸åªéœ€åœ¨åšå®¢ä½“ç³»åˆæœŸæˆ–é‡å¤§é‡æ„æ—¶è¿è¡Œä¸€æ¬¡ï¼›
# - ç”Ÿæˆçš„ YAML æ–‡ä»¶**å¿…é¡»ç»è¿‡äººå·¥å®¡æ ¸å’Œè°ƒæ•´**åå†ç”¨äºç¬¬äºŒé˜¶æ®µï¼ˆassign_categories_tags.pyï¼‰ï¼›
# - å¦‚éœ€å¤„ç†ä¸åŒåšå®¢ç›®å½•ï¼Œè¯·ä¿®æ”¹ä»£ç ä¸­çš„ BLOG_POSTS_DIR å˜é‡ï¼Œæˆ–å°†å…¶æ”¹ä¸ºå‘½ä»¤è¡Œå‚æ•°ï¼ˆå½“å‰ä¸ºç¡¬ç¼–ç ï¼‰ã€‚

ä¾èµ–ï¼š
  - Python åŒ…ï¼špyyaml, python-frontmatter, openai
  - DeepSeek API å¯†é’¥ï¼ˆå·²å†…ç½®ï¼Œå»ºè®®æ”¹ç”¨ç¯å¢ƒå˜é‡ç®¡ç†ï¼‰

æ­¤è„šæœ¬æ˜¯åšå®¢å…ƒæ•°æ®è‡ªåŠ¨åŒ– pipeline çš„ç¬¬ä¸€æ­¥ï¼Œä¸ºåç»­æ™ºèƒ½åˆ†ç±»æ‰“ä¸‹ç»“æ„åŒ–åŸºç¡€ã€‚
"""

import os
import re
import yaml
import time
from pathlib import Path
from typing import Any, Dict, List
from frontmatter import load as load_frontmatter
import openai

# ======================
# é…ç½®åŒºï¼ˆè¯·æŒ‰éœ€ä¿®æ”¹ï¼‰
# ======================
BLOG_POSTS_DIR = Path("blog/source/_posts")  # åšæ–‡æ ¹ç›®å½•
OUTPUT_CATEGORIES = "categories.yaml"
OUTPUT_TAGS = "tag_vocabulary.yaml"

# DeepSeek API é…ç½®ï¼ˆå…¼å®¹ OpenAIï¼‰
MODEL_NAME = "deepseek-reasoner"
os.environ["OPENAI_API_KEY"] = "sk-"
openai.base_url = "https://api.deepseek.com/v1/"
openai.api_key = os.getenv("OPENAI_API_KEY")

# å…¨å±€ç»Ÿè®¡å˜é‡
TOTAL_STATS = {
    "total_time": 0.0,
    "total_prompt_tokens": 0,
    "total_completion_tokens": 0,
    "total_requests": 0,
}


# ======================
# å·¥å…·å‡½æ•°
# ======================
def extract_lead_or_preview(content: str, max_chars=2000) -> str:
    if "<!-- more -->" in content:
        lead = content.split("<!-- more -->")[0].strip()
    else:
        lead = content[:max_chars]
    return lead.strip()


def clean_markdown_text(md_text: str) -> str:
    md_text = re.sub(r"```.*?```", "", md_text, flags=re.DOTALL)
    md_text = re.sub(r"`[^`]*`", "", md_text)
    md_text = re.sub(r"\[([^\]]+)\]\([^)]*\)", r"\1", md_text)
    md_text = re.sub(r"!\[[^\]]*\]\([^)]*\)", "", md_text)
    md_text = re.sub(r"[*_]{1,2}([^*_]+)[*_]{1,2}", r"\1", md_text)
    md_text = re.sub(r"^#+\s*", "", md_text, flags=re.MULTILINE)
    return md_text.strip()


def collect_articles() -> List[Dict]:
    articles = []
    for md_file in BLOG_POSTS_DIR.rglob("*.md"):
        try:
            with open(md_file, "r", encoding="utf-8") as f:
                post = load_frontmatter(f)
                content = post.content
                front_matter = dict(post)
                title = front_matter.get("title", md_file.stem)

                # å¤„ç† categories
                raw_cats = front_matter.get("categories", [])
                if isinstance(raw_cats, str):
                    existing_categories = [raw_cats]
                elif isinstance(raw_cats, list):
                    def flatten(lst):
                        result = []
                        for item in lst:
                            if isinstance(item, list):
                                result.extend(flatten(item))
                            else:
                                result.append(str(item).strip())
                        return result

                    existing_categories = flatten(raw_cats)
                else:
                    existing_categories = [str(raw_cats)]

                # å¤„ç† tags
                raw_tags = front_matter.get("tags", [])
                if isinstance(raw_tags, str):
                    existing_tags = [raw_tags]
                elif isinstance(raw_tags, list):
                    def flatten(lst):
                        result = []
                        for item in lst:
                            if isinstance(item, list):
                                result.extend(flatten(item))
                            else:
                                result.append(str(item).strip())
                        return result

                    existing_tags = flatten(raw_tags)
                else:
                    existing_tags = [str(raw_tags)]

                preview = extract_lead_or_preview(content, max_chars=2000)
                full_text = clean_markdown_text(content)

                articles.append({
                    "file": str(md_file),
                    "title": title,
                    "preview": preview,
                    "full_text": full_text,
                    "front_matter": front_matter,
                    "existing_categories": existing_categories,
                    "existing_tags": existing_tags,
                })
        except Exception as e:
            print(f"âš ï¸ è§£æå¤±è´¥: {md_file} - {e}")
    return articles


def log_llm_call(prompt: str, response: Any, start_time: float, task_name: str = "LLM Call") -> None:
    duration = time.time() - start_time
    usage = getattr(response, 'usage', None)
    prompt_tk = getattr(usage, 'prompt_tokens', 0)
    completion_tk = getattr(usage, 'completion_tokens', 0)

    TOTAL_STATS["total_prompt_tokens"] += prompt_tk
    TOTAL_STATS["total_completion_tokens"] += completion_tk
    TOTAL_STATS["total_requests"] += 1

    print(f"â±ï¸  {task_name} | è€—æ—¶: {duration:.2f}s | "
          f"è¾“å…¥: {prompt_tk} tk | è¾“å‡º: {completion_tk} tk")


def robust_llm_call(prompt: str, max_retries: int = 3, temperature: float = 0.3, max_tokens: int = 4096) -> str:
    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            start_time = time.time()
            response = openai.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            log_llm_call(prompt, response, start_time)
            raw_output = response.choices[0].message.content.strip()

            # ç§»é™¤å¯èƒ½çš„ markdown åŒ…è£¹
            if raw_output.startswith("```yaml"):
                raw_output = raw_output[7:]
            if raw_output.endswith("```"):
                raw_output = raw_output[:-3]
            return raw_output.strip()

        except Exception as e:
            last_error = e
            wait_time = min(2 ** attempt, 10)  # æœ€å¤šç­‰ 10 ç§’
            print(f"âš ï¸  ç¬¬ {attempt}/{max_retries} æ¬¡è°ƒç”¨å¤±è´¥: {e}. ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)

    raise RuntimeError(f"LLM è°ƒç”¨å¤±è´¥ï¼ˆ{max_retries} æ¬¡é‡è¯•åä»å¤±è´¥ï¼‰: {last_error}")


# ======================
# ä»»åŠ¡ A: ç”Ÿæˆ Category ä½“ç³»
# ======================
def build_category_prompt(articles: List[Dict]) -> str:
    snippets = []
    for art in articles:
        fm = art["front_matter"]
        meta_lines = []
        if fm.get("temporal_type"):
            meta_lines.append(f"temporal_type: {fm['temporal_type']}")
        if fm.get("intent"):
            meta_lines.append(f"intent: {fm['intent']}")
        if art["existing_categories"]:
            meta_lines.append(f"å½“å‰åˆ†ç±»: {', '.join(art['existing_categories'])}")
        if art["existing_tags"]:
            meta_lines.append(f"å½“å‰æ ‡ç­¾: {', '.join(art['existing_tags'][:5])}")

        meta_str = "\n".join(meta_lines) if meta_lines else "æ— "

        snippet = (
            f"æ ‡é¢˜: {art['title']}\n"
            f"å†…å®¹é¢„è§ˆ:\n{art['preview'][:2000]}\n"
            f"å…ƒä¿¡æ¯ä¸ç°æœ‰æ ‡æ³¨:\n{meta_str}"
        ).strip()
        snippets.append(snippet)

    article_snippets = "\n\n---\n\n".join(snippets)

    return f"""ä½ æ˜¯ä¸€ä½åšå®¢æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š**åŸºäºä»¥ä¸‹æ‰€æœ‰æ–‡ç« çš„ç°æœ‰åˆ†ç±»ã€æ ‡ç­¾ã€å†…å®¹å’Œå…ƒä¿¡æ¯ï¼Œè®¾è®¡ä¸€ä¸ªç»Ÿä¸€ã€åˆç†ã€ç»“æ„æ¸…æ™°çš„é¢„å®šä¹‰åˆ†ç±»ä½“ç³»ï¼ˆCategory Hierarchyï¼‰**ã€‚

èƒŒæ™¯ï¼š
- æ¯ç¯‡æ–‡ç« å·²æœ‰ `categories` å’Œ `tags`ï¼Œä½†å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´ã€å†—ä½™æˆ–ç²’åº¦ä¸åˆç†çš„é—®é¢˜ã€‚
- ä½ éœ€è¦**å½’çº³å‡ºä¸€å¥—æ–°çš„ã€æ ‡å‡†åŒ–çš„ category ä½“ç³»**ï¼Œç”¨äºæœªæ¥æ‰€æœ‰æ–‡ç« çš„è‡ªåŠ¨åˆ†ç±»ã€‚

è¦æ±‚ï¼š
1. **Category å¿…é¡»åæ˜ æ–‡ç« çš„å†™ä½œç›®çš„æˆ–æ–‡ä½“ç±»å‹**ï¼ˆå¦‚å¹´åº¦æ€»ç»“ã€æ•™ç¨‹ã€é¡¹ç›®å¤ç›˜ï¼‰ï¼Œ**ä¸æ˜¯æŠ€æœ¯ä¸»é¢˜**ã€‚
   - âœ… æ­£ç¡®ç¤ºä¾‹ï¼š["åšå®¢", "å¹´åº¦æ€»ç»“"]ã€["æ•™ç¨‹", "å·¥å…·ä½¿ç”¨"]
   - âŒ é”™è¯¯ç¤ºä¾‹ï¼š["æ·±åº¦å­¦ä¹ "]ã€["è®¡ç®—æœºè§†è§‰"]ï¼ˆè¿™æ˜¯ tag çš„èŒƒç•´ï¼‰
2. ä½¿ç”¨ä¸¤çº§ç»“æ„ï¼š[å¤§ç±», å­ç±»]
3. ä¸ºæ¯ä¸ª category æä¾›ï¼š
   - `path`: [å¤§ç±», å­ç±»]
   - `description`: ä¸€å¥è¯è¯´æ˜é€‚ç”¨åœºæ™¯
   - `matching_hints`: 3~5 ä¸ªå…³é”®è¯ã€ä¿¡å·æˆ–è§„åˆ™ï¼ˆç”¨äºåç»­è‡ªåŠ¨åŒ¹é…ï¼‰
4. å¤§ç±»ä¸è¶…è¿‡ 20 ä¸ªã€‚

ä»¥ä¸‹æ˜¯å…¨éƒ¨æ–‡ç« çš„ä¿¡æ¯ï¼š
---
{article_snippets}
---

è¯·ä»¥ YAML æ ¼å¼è¾“å‡ºï¼Œé¡¶å±‚ä¸º `categories`ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« path/description/matching_hintsã€‚
ä¸è¦ä»»ä½•è§£é‡Šï¼Œä¸è¦ markdown ä»£ç å—ï¼Œç›´æ¥è¾“å‡º YAML å†…å®¹ã€‚
"""


def run_task_a(articles: List[Dict]):
    start_time = time.time()
    raw_output = ""
    try:
        prompt = build_category_prompt(articles)
        raw_output = robust_llm_call(prompt, max_retries=3, temperature=0.3, max_tokens=65536)

        data = yaml.safe_load(raw_output)
        with open(OUTPUT_CATEGORIES, "w", encoding="utf-8") as f:
            yaml.dump(data, f, allow_unicode=True, indent=2, sort_keys=False)
        print(f"âœ… ä»»åŠ¡ A å®Œæˆï¼šå·²ç”Ÿæˆ {OUTPUT_CATEGORIES}")
    except Exception as e:
        print(f"âŒ ä»»åŠ¡ A æœ€ç»ˆå¤±è´¥: {e}")
        with open(OUTPUT_CATEGORIES + ".raw", "w", encoding="utf-8") as f:
            f.write(raw_output)
        raise e
    finally:
        task_time = time.time() - start_time
        TOTAL_STATS["total_time"] += task_time
        print(f"ğŸ“Š ä»»åŠ¡ A æ€»è€—æ—¶: {task_time:.2f} ç§’")


# ======================
# ä»»åŠ¡ B: ç”Ÿæˆ Tag è¯è¡¨
# ======================
def robust_llm_yaml_call(
        prompt: str,
        max_retries: int = 5,
        temperature: float = 0.3,
        max_tokens: int = 65536,
        expected_type: str = "list"
) -> List[Dict]:
    """
    è°ƒç”¨ LLM å¹¶ç¡®ä¿è¿”å›å†…å®¹èƒ½è¢«è§£æä¸ºåˆæ³• YAMLï¼Œä¸”ç»“æ„ç¬¦åˆé¢„æœŸã€‚
    """
    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            # è°ƒç”¨ LLM
            raw_output = robust_llm_call(
                prompt=prompt,
                max_retries=1,  # å†…éƒ¨ä¸å†é‡è¯•ç½‘ç»œé”™è¯¯ï¼Œç”±å¤–å±‚æ§åˆ¶
                temperature=temperature,
                max_tokens=max_tokens
            )

            # å°è¯•è§£æ YAML
            parsed = yaml.safe_load(raw_output)
            if parsed is None:
                raise ValueError("YAML è§£æç»“æœä¸º None")

            # éªŒè¯ç»“æ„
            if expected_type == "list":
                if not isinstance(parsed, list):
                    raise ValueError(f"æœŸæœ›è¾“å‡ºä¸ºåˆ—è¡¨ï¼Œä½†å¾—åˆ° {type(parsed)}")
                # ç®€å•éªŒè¯æ¯ä¸ªå…ƒç´ æ˜¯ dict ä¸”å« standard_tag
                for item in parsed:
                    if not isinstance(item, dict) or "standard_tag" not in item:
                        raise ValueError("åˆ—è¡¨ä¸­å­˜åœ¨éæ ‡å‡†æ¡ç›®")
            elif expected_type == "dict_with_tags":
                if not (isinstance(parsed, dict) and "tags" in parsed):
                    raise ValueError("æœŸæœ›å­—å…¸åŒ…å« 'tags' é”®")
                parsed = parsed["tags"]

            return parsed  # æˆåŠŸï¼

        except (yaml.YAMLError, ValueError, TypeError) as e:
            last_error = e
            print(f"âš ï¸  ç¬¬ {attempt}/{max_retries} æ¬¡ YAML è§£æå¤±è´¥: {e}")
            if attempt < max_retries:
                # å¯é€‰ï¼šåœ¨ä¸‹ä¸€æ¬¡ prompt ä¸­åŠ å…¥çº é”™æŒ‡ä»¤
                prompt = f"""ä¹‹å‰çš„è¾“å‡ºæ— æ³•è¢«è§£æä¸ºåˆæ³• YAMLã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™é‡æ–°ç”Ÿæˆï¼š

- å¿…é¡»æ˜¯çº¯ YAMLï¼Œæ— ä»»ä½•è§£é‡Šã€æ ‡é¢˜æˆ– markdown åŒ…è£¹
- æ¯ä¸ªæ¡ç›®æ ¼å¼ï¼š
    - standard_tag: æ ‡å‡†åç§°
      aliases:
        - åˆ«å1
        - åˆ«å2
- ä¸è¦ä½¿ç”¨å†…è”åˆ—è¡¨å¦‚ [a, b]
- ä¸è¦çœç•¥ç¼©è¿›

ç°åœ¨è¯·é‡æ–°å¤„ç†ä»¥ä¸‹å†…å®¹ï¼š
{prompt.split('æ–‡ç« å†…å®¹å¦‚ä¸‹ï¼š')[-1]}"""
                time.sleep(min(2 ** attempt, 8))
            else:
                break

    raise RuntimeError(f"YAML ç”Ÿæˆå¤±è´¥ï¼ˆ{max_retries} æ¬¡é‡è¯•åä»æ— æ•ˆï¼‰: {last_error}")


def build_tag_prompt(articles: List[Dict]) -> str:
    snippets = []
    for art in articles:
        short_text = art['full_text'][:2000].replace("\n", " ")
        snippets.append(f"æ–‡ç« : {art['title']}\nå†…å®¹: {short_text}")
    all_texts = "\n\n---\n\n".join(snippets)
    return f"""ä½ æ˜¯ä¸€ä½æŠ€æœ¯æœ¯è¯­æ•´ç†ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¤šç¯‡æŠ€æœ¯åšå®¢ä¸­ï¼Œæå–æ‰€æœ‰**å…·ä½“ã€å¯æ£€ç´¢çš„æŠ€æœ¯å®ä½“**ï¼Œå¹¶æ„å»ºä¸€ä¸ªæ ‡å‡†åŒ–æ ‡ç­¾è¯è¡¨ã€‚

è¦æ±‚ï¼š
1. åªæå–å…·ä½“åè¯ï¼šå·¥å…·ã€æ¡†æ¶ã€è¯­è¨€ã€ç®—æ³•ã€åè®®ã€é¡¹ç›®åã€å¹´ä»½ã€æ–¹æ³•è®ºç­‰ã€‚
   - âœ… ä¾‹å¦‚ï¼šDocker, YOLOv5, git, 2019, ResNet, CI/CD, LabelImage
   - âŒ æ’é™¤ï¼šå­¦ä¹ ã€å·¥ä½œã€æ„Ÿè§‰ã€æé«˜ã€é—®é¢˜ï¼ˆå¤ªæ³›ï¼‰
2. åˆå¹¶åŒä¹‰è¯/å˜ä½“ï¼Œä¸ºæ¯ä¸ªæ ‡å‡†æ ‡ç­¾åˆ—å‡ºå¸¸è§åˆ«åã€‚
3. è¾“å‡ºæ ¼å¼ä¸º YAML åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡ç›®ï¼š
   - standard_tag: æ ‡å‡†å½¢å¼
   - aliases: [å˜ä½“åˆ—è¡¨]

æ–‡ç« å†…å®¹å¦‚ä¸‹ï¼š
---
{all_texts}
---

è¯·ç›´æ¥è¾“å‡º YAML åˆ—è¡¨ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ– markdown åŒ…è£¹ã€‚
"""


def run_task_b(articles: List[Dict], batch_size: int = 50):
    all_tag_entries = []
    start_time = time.time()

    for i in range(0, len(articles), batch_size):
        batch = articles[i:i + batch_size]
        batch_index = i // batch_size + 1
        print(f"ğŸ“¦ å¤„ç†ç¬¬ {batch_index} æ‰¹ï¼Œå…± {len(batch)} ç¯‡æ–‡ç« ...")

        prompt = build_tag_prompt(batch)
        try:
            parsed_list = robust_llm_yaml_call(
                prompt=prompt,
                max_retries=5,
                temperature=0.3,
                max_tokens=65536,
                expected_type="list"
            )
            all_tag_entries.extend(parsed_list)
            print(f"âœ… ç¬¬ {batch_index} æ‰¹æˆåŠŸè§£æ {len(parsed_list)} ä¸ªæ ‡ç­¾")

        except Exception as e:
            # å³ä½¿é‡è¯• 5 æ¬¡ä»å¤±è´¥ï¼Œè®°å½•åŸå§‹è¾“å‡ºä¾›äººå·¥æ£€æŸ¥ï¼Œä½†ä¸è·³è¿‡ï¼
            print(f"ğŸ’¥ ç¬¬ {batch_index} æ‰¹å½»åº•å¤±è´¥ï¼ˆæ‰€æœ‰é‡è¯•å‡æ— æ•ˆï¼‰: {e}")
            # ä¿å­˜åŸå§‹è¾“å‡ºç”¨äºè°ƒè¯•
            raw_debug_file = f"tag_batch_{i}.raw"
            with open(raw_debug_file, "w", encoding="utf-8") as f:
                f.write(prompt)  # æˆ–è€…ä½ å¯ä»¥ä¿å­˜æœ€åä¸€æ¬¡ raw_outputï¼ˆéœ€è°ƒæ•´å‡½æ•°ï¼‰
            print(f"   âš ï¸ å·²ä¿å­˜è°ƒè¯•æ–‡ä»¶: {raw_debug_file}")
            # æ³¨æ„ï¼šè¿™é‡Œä»ç„¶è·³è¿‡ï¼Œå› ä¸ºå®åœ¨æ— æ³•è§£æã€‚ä½†æ¦‚ç‡æä½ã€‚
            continue

    # åˆå¹¶å»é‡
    tag_dict = {}
    for entry in all_tag_entries:
        if not isinstance(entry, dict):
            continue
        std_tag = entry.get("standard_tag")
        aliases = entry.get("aliases", [])
        if not std_tag:
            continue
        std_tag = str(std_tag).strip()
        if std_tag not in tag_dict:
            tag_dict[std_tag] = set()
        for alias in aliases:
            tag_dict[std_tag].add(str(alias).strip())
        tag_dict[std_tag].add(std_tag)

    final_tags = [
        {"standard_tag": std, "aliases": sorted(list(aliases))}
        for std, aliases in tag_dict.items()
    ]

    with open(OUTPUT_TAGS, "w", encoding="utf-8") as f:
        yaml.dump({"tags": final_tags}, f, allow_unicode=True, indent=2, sort_keys=False)

    task_time = time.time() - start_time
    TOTAL_STATS["total_time"] += task_time
    print(f"âœ… ä»»åŠ¡ B å®Œæˆï¼šå·²ç”Ÿæˆ {OUTPUT_TAGS}ï¼ˆå…± {len(final_tags)} ä¸ªæ ‡å‡†æ ‡ç­¾ï¼‰")
    print(f"ğŸ“Š ä»»åŠ¡ B æ€»è€—æ—¶: {task_time:.2f} ç§’")


# ======================
# ä¸»æµç¨‹
# ======================
if __name__ == "__main__":
    print("ğŸ” æ­£åœ¨æ‰«æåšå®¢æ–‡ç« ...")
    articles = collect_articles()
    print(f"ğŸ“š å…±æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« \n")

    try:
        print("ğŸš€ æ‰§è¡Œä»»åŠ¡ Aï¼šç”Ÿæˆé¢„å®šä¹‰ Category ä½“ç³»...")
        run_task_a(articles)

        print("\nğŸš€ æ‰§è¡Œä»»åŠ¡ Bï¼šç”Ÿæˆæ ‡å‡†åŒ– Tag è¯è¡¨...")
        run_task_b(articles)

        print("\nğŸ‰ ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼è¯·æ£€æŸ¥ç”Ÿæˆçš„ YAML æ–‡ä»¶å¹¶è¿›è¡Œäººå·¥å®¡æ ¸ã€‚")
    finally:
        print("\n" + "=" * 60)
        print("ğŸ“ˆ å…¨å±€ç»Ÿè®¡:")
        print(f"  â€¢ æ€»è€—æ—¶: {TOTAL_STATS['total_time']:.2f} ç§’")
        print(f"  â€¢ æ€»è¯·æ±‚æ•°: {TOTAL_STATS['total_requests']}")
        print(f"  â€¢ æ€»è¾“å…¥ tokens: {TOTAL_STATS['total_prompt_tokens']}")
        print(f"  â€¢ æ€»è¾“å‡º tokens: {TOTAL_STATS['total_completion_tokens']}")
        print("=" * 60)
