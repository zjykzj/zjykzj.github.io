# -*- coding: utf-8 -*-

"""
@Time    : 2025/12/24 21:02
@File    : stage2_build_ontology.py
@Author  : zj
@Description:

Stage 2: æ„å»ºç»Ÿä¸€æ ‡ç­¾ & åˆ†ç±»ä½“ç³»ï¼ˆæ”¯æŒæ¯ç¯‡æ–‡ç« æœ€å¤š 3 ä¸ª [ä¸»ç±», å­ç±»] ç±»åˆ«ï¼‰

è¾“å…¥: .tmp/*.json ï¼ˆæ¥è‡ª stage1ï¼‰
è¾“å‡º:
  - .ontology/tags.json        # æ ‡å‡†åŒ–æ ‡ç­¾ + åˆ«å
  - .ontology/categories.json  # å…¨å±€åˆ†ç±»ä½“ç³»ï¼ˆ[ä¸»ç±», å­ç±»] åˆ—è¡¨ï¼‰
  - .ontology/assignment.json  # æ¯ç¯‡æ–‡ç« çš„ file / categories / tags

ç±»åˆ«è¯´æ˜:
  - æ¯ä¸ªç±»åˆ«æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ—è¡¨: ["ä¸»ç±»", "å­ç±»"]
  - æ¯ç¯‡æ–‡ç« æœ€å¤šåˆ†é… 3 ä¸ªè¿™æ ·çš„ç±»åˆ«
  - ä¸»ç±»åæ˜ æ–‡ç« æ€§è´¨ï¼ˆå¦‚"è®ºæ–‡è§£è¯»"ï¼‰ï¼Œå­ç±»åæ˜ é¢†åŸŸæˆ–ç›®æ ‡ï¼ˆå¦‚"è®¡ç®—æœºè§†è§‰"ï¼‰

ç›®æ ‡ï¼š
1. æ ‡ç­¾æ ‡å‡†åŒ–ï¼šåˆå¹¶åŒä¹‰æ ‡ç­¾ï¼Œè¾“å‡º tags.json
   - æ¯ä¸ªæ ‡å‡†æ ‡ç­¾å½¢å¦‚ "ä¸­æ–‡/English"
   - è®°å½•æ‰€æœ‰åˆ«å
2. åˆ†ç±»ä½“ç³»æ„å»ºï¼šåŸºäº content_type + main_ideaï¼Œè¾“å‡º categories.json
   - èšç„¦â€œå†™ä½œç›®çš„â€è€ŒéæŠ€æœ¯ä¸»é¢˜
3. ä¸ºæ¯ç¯‡æ–‡ç« åˆ†é…æœ€ç»ˆ categories å’Œ tagsï¼Œè¾“å‡º assignment.json

æ³¨æ„ï¼šä¸åæƒœ tokensï¼Œå¤šæ¬¡è°ƒç”¨ LLMï¼Œç¡®ä¿è´¨é‡ã€‚
"""

import json
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict
import openai
import time
import sys

# === é…ç½® ===
TMP_DIR = Path(".tmp")
OUTPUT_DIR = Path(".ontology")
OUTPUT_DIR.mkdir(exist_ok=True)

MODEL_NAME = "deepseek-chat"
openai.base_url = "https://api.deepseek.com/v1/"
openai.api_key = "sk-"

# === å…¨å±€ç»Ÿè®¡ ===
STATS = {
    'total_calls': 0,
    'total_prompt_tokens': 0,
    'total_completion_tokens': 0,
}


def call_llm(prompt: str, temperature: float = 0.3, max_tokens: int = 4096) -> str:
    """è°ƒç”¨ LLM å¹¶æ›´æ–°ç»Ÿè®¡"""
    for attempt in range(3):
        try:
            start = time.time()
            resp = openai.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=120
            )
            duration = time.time() - start
            content = resp.choices[0].message.content.strip()

            usage = getattr(resp, 'usage', None)
            pt = getattr(usage, 'prompt_tokens', 0)
            ct = getattr(usage, 'completion_tokens', 0)
            STATS['total_calls'] += 1
            STATS['total_prompt_tokens'] += pt
            STATS['total_completion_tokens'] += ct

            print(f"  âœ… LLM æˆåŠŸ | è€—æ—¶: {duration:.1f}s | Tokens: {pt}/{ct}")
            return content
        except Exception as e:
            wait = 2 ** attempt
            print(f"  âš ï¸ é‡è¯• {attempt + 1}/3: {e}, ç­‰å¾… {wait}s...", file=sys.stderr)
            time.sleep(wait)
    raise RuntimeError("LLM è°ƒç”¨å¤±è´¥")


def load_all_meta() -> Dict[str, Dict]:
    """åŠ è½½æ‰€æœ‰ .tmp/*.json æ–‡ä»¶"""
    metas = {}
    for f in TMP_DIR.glob("*.json"):
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                data = json.load(fp)
                if isinstance(data, dict) and 'error' not in data:
                    metas[f.stem] = data
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æŸåæ–‡ä»¶ {f.name}: {e}", file=sys.stderr)
    print(f"âœ… åŠ è½½ {len(metas)} ç¯‡æœ‰æ•ˆæ–‡ç« å…ƒæ•°æ®")
    return metas


# ======================
# ç¬¬ä¸€æ­¥ï¼šæ ‡ç­¾æ ‡å‡†åŒ–
# ======================

def extract_tag_parts_fallback(tag: str) -> Tuple[str, str]:
    """
    ä»…åšæ ¼å¼è§£æï¼Œä¸åšè¯­ä¹‰å½’ä¸€ã€‚
    ç”¨äº fallback æ„é€  standard_tagã€‚
    """
    tag = tag.strip()
    if '/' in tag:
        parts = [p.strip() for p in tag.split('/', 1)]
        zh = parts[0] or parts[1]
        en = parts[1] or parts[0]
    else:
        # å¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œè§†ä¸ºä¸­æ–‡
        if any('\u4e00' <= c <= '\u9fff' for c in tag):
            zh, en = tag, ""
        else:
            zh, en = "", tag
    return zh, en


def normalize_tags(all_raw_tags: List[str]) -> List[Dict[str, Any]]:
    """
    é«˜ç²¾åº¦æ ‡ç­¾æ ‡å‡†åŒ–æµç¨‹ï¼ˆä¸‰é˜¶æ®µï¼‰ï¼š

    1. ã€è¯­ä¹‰èšç±»ã€‘å°†åŸå§‹æ ‡ç­¾æŒ‰è¯­ä¹‰åˆ†ç»„ï¼ˆæ¯æ‰¹ â‰¤30 ä¸ªï¼Œè°ƒç”¨ LLM èšç±»ï¼‰
    2. ã€ç°‡æ ‡å‡†åŒ–ã€‘å¯¹æ¯ä¸ªè¯­ä¹‰ç°‡é€‰å‡º standard_tag å¹¶è®°å½• aliases
    3. ã€å…¨å±€å»é‡ã€‘åˆå¹¶è·¨ç°‡å†²çªï¼ˆå¦‚ Aâ†’B å’Œ Bâ†’C åˆå¹¶ä¸º A,B,Câ†’ç»Ÿä¸€æ ‡å‡†ï¼‰

    è¾“å…¥ï¼šæ‰€æœ‰åŸå§‹æ ‡ç­¾ï¼ˆå¯èƒ½å«é‡å¤ã€ç©ºæ ¼ã€å¤§å°å†™å˜ä½“ï¼‰
    è¾“å‡ºï¼š[{"standard_tag": "ä¸­æ–‡/English", "aliases": [...]}]

    æ³¨æ„ï¼šä¸ä¾èµ–é¢„å®šä¹‰è¯å…¸ï¼Œå®Œå…¨ç”± LLM é©±åŠ¨è¯­ä¹‰ç†è§£ã€‚
    """
    unique_tags = sorted({t.strip() for t in all_raw_tags if t.strip()})
    if not unique_tags:
        return []

    print(f"ğŸ·ï¸ å…± {len(unique_tags)} ä¸ªå”¯ä¸€åŸå§‹æ ‡ç­¾ï¼Œå¼€å§‹çº¯è¯­ä¹‰æ ‡å‡†åŒ–...")

    # ==============================
    # é˜¶æ®µ 1: åˆ†æ‰¹èšç±»ï¼ˆæ¯æ‰¹æœ€å¤š 30 ä¸ªæ ‡ç­¾ï¼‰
    # ==============================
    all_clusters: List[List[str]] = []
    batch_size = 30

    # è®¡ç®—æ€»æ‰¹æ¬¡æ•°ï¼Œç”¨äºè¿›åº¦æ˜¾ç¤º
    total_batches = (len(unique_tags) + batch_size - 1) // batch_size

    for i in range(0, len(unique_tags), batch_size):
        batch_index = i // batch_size + 1  # ä» 1 å¼€å§‹è®¡æ•°
        batch = unique_tags[i:i + batch_size]
        if len(batch) == 1:
            all_clusters.append(batch)
            print(f"[{batch_index}/{total_batches}] è·³è¿‡å•æ ‡ç­¾æ‰¹æ¬¡: {batch[0]}")
            continue

        tags_str = "\n".join(f"- {tag}" for tag in batch)
        prompt = f"""ä½ æ˜¯ä¸€ä½æåº¦ä¸¥è°¨çš„æŠ€æœ¯æœ¯è¯­æ ‡å‡†åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š**ä»…å½“ä¸¤ä¸ªæˆ–å¤šä¸ªæ ‡ç­¾å®Œå…¨ç­‰ä»·æ—¶æ‰å°†å®ƒä»¬å½’ä¸ºä¸€ç»„**ï¼Œå¦åˆ™æ¯ä¸ªæ ‡ç­¾å¿…é¡»å•ç‹¬æˆç»„ã€‚

        ğŸ“Œ åˆå¹¶çš„å”¯ä¸€åˆæ³•æƒ…å½¢ï¼ˆå¿…é¡»æ»¡è¶³ä»¥ä¸‹ä¹‹ä¸€ï¼‰ï¼š
        - å®Œå…¨ç›¸åŒçš„æœ¯è¯­ï¼Œä»…æ ¼å¼ä¸åŒï¼ˆå¦‚å¤§å°å†™ã€è¿å­—ç¬¦ã€ç©ºæ ¼ï¼‰ï¼š["ResNet50", "ResNet-50"]
        - å®˜æ–¹åŒä¹‰è¯æˆ–åˆ«åï¼ˆå¦‚ "BERT" å’Œ "Bidirectional Encoder Representations from Transformers"ï¼‰
        - ä¸­è‹±æ–‡å¯¹ç…§ä¸”æ˜ç¡®æŒ‡ä»£åŒä¸€äº‹ç‰©ï¼š["å·ç§¯ç¥ç»ç½‘ç»œ", "CNN"]
        - ç¼©å†™ä¸å…¨ç§°ä¸€ä¸€å¯¹åº”ï¼š["LLM", "Large Language Model"]

        ğŸš« ä»¥ä¸‹æƒ…å†µ**ç»å¯¹ç¦æ­¢åˆå¹¶**ï¼ˆå³ä½¿çœ‹èµ·æ¥ç›¸å…³ï¼‰ï¼š
        - ä¸åŒç®—æ³•ï¼ˆå¦‚ Adam â‰  AdaGrad â‰  RMSPropï¼‰
        - ä¸åŒæ•°æ®é›†ï¼ˆå¦‚ CIFAR-10 â‰  ImageNet â‰  COCOï¼‰
        - ä¸åŒæ¶æ„ï¼ˆå¦‚ AlexNet â‰  ResNet â‰  Transformerï¼‰
        - ä¸åŒæ¦‚å¿µä½†é¦–å­—æ¯ç›¸åŒï¼ˆå¦‚ API â‰  AML â‰  AP@nï¼‰
        - ä¸€ä¸ªé€šç”¨è¯ + ä¸€ä¸ªå…·ä½“å·¥å…·ï¼ˆå¦‚ "AIè¯†åˆ«" â‰  "ARM G52"ï¼‰
        - ä»»ä½•ä½ æ— æ³• 100% ç¡®å®šå¯äº’æ¢çš„æƒ…å†µ

        âš ï¸ é‡è¦æŒ‡ä»¤ï¼š
        - å¦‚æœä½ å¯¹ä»»æ„ä¸¤ä¸ªæ ‡ç­¾æ˜¯å¦ç­‰ä»·å­˜åœ¨**ä¸æ¯«çŠ¹è±«ï¼Œè¯·å°†å®ƒä»¬åˆ†å¼€**
        - å®å¯è¾“å‡º 30 ä¸ªå•å…ƒç´ ç»„ï¼Œä¹Ÿä¸è¦è¾“å‡º 1 ä¸ªé”™è¯¯åˆå¹¶ç»„
        - æ¯ä¸ªæ ‡ç­¾å¿…é¡»å‡ºç°åœ¨ä¸”ä»…å‡ºç°åœ¨ä¸€ä¸ªç»„ä¸­

        æ ‡ç­¾åˆ—è¡¨ï¼ˆå…± {len(batch)} ä¸ªï¼‰ï¼š
        {tags_str}

        è¾“å‡ºæ ¼å¼ï¼šä¸¥æ ¼ä¸º JSON åˆ—è¡¨ï¼Œæ¯ç»„æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æ•°ç»„ã€‚
        âœ… æ­£ç¡®ç¤ºä¾‹ï¼š
        [
          ["LLM", "Large Language Model"],
          ["ResNet50"],
          ["Adam"],
          ["AdaGrad"]
        ]
        âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆä¸è¦è¿™æ ·åšï¼‰ï¼š
        [
          ["Adam", "AdaGrad"],  // ä¸åŒä¼˜åŒ–å™¨ï¼
          ["API", "AML"]       // å®Œå…¨æ— å…³ï¼
        ]

        ç°åœ¨ï¼Œè¯·è¾“å‡ºèšç±»ç»“æœï¼ˆä»… JSONï¼Œæ— å…¶ä»–å†…å®¹ï¼‰ï¼š
        """

        print(
            f"[{batch_index}/{total_batches}] ğŸ§  LLM èšç±»æ‰¹æ¬¡ï¼ˆ{len(batch)} ä¸ªæ ‡ç­¾ï¼‰: {batch[:3]}{'...' if len(batch) > 3 else ''}")
        try:
            output = call_llm(prompt, temperature=0.0, max_tokens=1024)
            if output.startswith("```json"):
                output = output[7:-3].strip()
            elif output.startswith("```"):
                output = output[3:-3].strip()
            clusters = json.loads(output)

            covered = set()
            valid_clusters = []
            for cluster in clusters:
                if not isinstance(cluster, list):
                    continue
                clean_cluster = []
                for tag in cluster:
                    if tag in batch and tag not in covered:
                        clean_cluster.append(tag)
                        covered.add(tag)
                if clean_cluster:
                    valid_clusters.append(clean_cluster)
            missing = [tag for tag in batch if tag not in covered]
            for tag in missing:
                valid_clusters.append([tag])
            all_clusters.extend(valid_clusters)
            print(f"  âœ… æ‰¹æ¬¡ {batch_index} èšç±»æˆåŠŸï¼Œç”Ÿæˆ {len(valid_clusters)} ä¸ªç°‡")

            # === æ–°å¢ï¼šæ‰“å°åˆå¹¶çš„è¯­ä¹‰ç°‡ï¼ˆä»…æ˜¾ç¤ºé•¿åº¦ â‰¥2 çš„ï¼‰===
            merged_clusters = [c for c in valid_clusters if len(c) > 1]
            if merged_clusters:
                print(f"    ğŸ”— åˆå¹¶çš„åŒä¹‰æ ‡ç­¾ç»„ï¼ˆå…± {len(merged_clusters)} ç»„ï¼‰:")
                for cid, cluster in enumerate(merged_clusters, 1):
                    print(f"      {cid}. {cluster}")
            else:
                print("    â– æ— åˆå¹¶ï¼Œæ‰€æœ‰æ ‡ç­¾ç‹¬ç«‹æˆç°‡")
        except Exception as e:
            print(f"âš ï¸ æ‰¹æ¬¡èšç±»å¤±è´¥ï¼ˆ{len(batch)} ä¸ªæ ‡ç­¾ï¼‰ï¼Œå›é€€ä¸ºå•æ ‡ç­¾ç»„: {e}")
            for tag in batch:
                all_clusters.append([tag])

    print(f"âœ… èšç±»å®Œæˆï¼Œå¾—åˆ° {len(all_clusters)} ä¸ªè¯­ä¹‰ç°‡")

    print(f"ğŸ“Š ç°‡å¤§å°åˆ†å¸ƒ:")
    size_count = defaultdict(int)
    for c in all_clusters:
        size_count[len(c)] += 1
    for size in sorted(size_count):
        print(f"  é•¿åº¦ {size}: {size_count[size]} ä¸ªç°‡")

    # ==============================
    # é˜¶æ®µ 2: å¯¹æ¯ä¸ªç°‡æ ‡å‡†åŒ–
    # ==============================
    standardized = []

    for idx, cluster in enumerate(all_clusters, 1):
        print(f"[{idx}/{len(all_clusters)}] æ ‡å‡†åŒ–ç°‡: {cluster[:3]}{'...' if len(cluster) > 3 else ''}")
        cluster = list(dict.fromkeys(cluster))
        if len(cluster) == 1:
            tag = cluster[0]
            zh, en = extract_tag_parts_fallback(tag)
            std_tag = f"{zh}/{en}" if zh and en else (zh or en or tag)
            standardized.append({
                "standard_tag": std_tag,
                "aliases": []
            })
            continue

        if len(cluster) > 20:
            for j in range(0, len(cluster), 20):
                sub = cluster[j:j + 20]
                _standardize_cluster(sub, standardized)
        else:
            _standardize_cluster(cluster, standardized)

    # ==============================
    # é˜¶æ®µ 3: å…¨å±€å»é‡ & å†²çªè§£å†³
    # ==============================
    std_to_aliases: Dict[str, Set[str]] = defaultdict(set)
    tag_to_std: Dict[str, str] = {}

    for item in standardized:
        std = item["standard_tag"]
        aliases = item["aliases"]
        alias_set = {a for a in aliases if a != std}
        std_to_aliases[std].update(alias_set)
        tag_to_std[std] = std
        for a in alias_set:
            if a in tag_to_std and tag_to_std[a] != std:
                old_std = tag_to_std[a]
                std_to_aliases[std].update(std_to_aliases[old_std])
                std_to_aliases[std].discard(std)
                for t in [old_std] + list(std_to_aliases[old_std]):
                    tag_to_std[t] = std
                std_to_aliases.pop(old_std, None)
            else:
                tag_to_std[a] = std

    final_result = []
    emitted_std = set()
    for std, aliases in std_to_aliases.items():
        if std in emitted_std:
            continue
        emitted_std.add(std)
        clean_aliases = [a for a in aliases if a not in std_to_aliases]
        final_result.append({
            "standard_tag": std,
            "aliases": clean_aliases
        })

    all_covered = set(tag_to_std.keys())
    missing = [t for t in unique_tags if t not in all_covered]
    for t in missing:
        zh, en = extract_tag_parts_fallback(t)
        std_tag = f"{zh}/{en}" if zh and en else (zh or en or t)
        final_result.append({
            "standard_tag": std_tag,
            "aliases": []
        })

    print(f"ğŸ¯ æ ‡ç­¾æ ‡å‡†åŒ–å®Œæˆï¼šå…± {len(final_result)} ä¸ªæ ‡å‡†æ ‡ç­¾")
    return final_result


def _standardize_cluster(cluster: List[str], result_list: List[Dict[str, Any]]):
    tags_str = "\n".join(f"- {tag}" for tag in cluster)
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ç»„è¯­ä¹‰ç›¸åŒæˆ–é«˜åº¦ç›¸å…³çš„æ ‡ç­¾ã€‚è¯·é€‰æ‹©å…¶ä¸­ä¸€ä¸ªä½œä¸º standard_tagï¼Œ
è¦æ±‚ï¼š
1. ä¼˜å…ˆé€‰æ‹©åŒ…å«ä¸­è‹±æ–‡çš„å½¢å¼ï¼ˆå¦‚â€œå¤§æ¨¡å‹/Large Language Modelâ€ï¼‰
2. è‹¥æ— åŒè¯­ï¼Œé€‰æœ€å®Œæ•´ã€è§„èŒƒçš„å½¢å¼
3. standard_tag å¿…é¡»æ˜¯ä»¥ä¸‹åˆ—è¡¨ä¸­çš„ä¸€ä¸ª

æ ‡ç­¾åˆ—è¡¨ï¼š
{tags_str}

è¾“å‡ºä¸¥æ ¼ä¸º JSONï¼š
{{
  "standard_tag": "é€‰ä¸­çš„æ ‡ç­¾",
  "aliases": ["å…¶ä½™æ ‡ç­¾"]
}}
æ³¨æ„ï¼šaliases ä¸å¾—åŒ…å« standard_tag æœ¬èº«ã€‚
"""

    try:
        output = call_llm(prompt, temperature=0.1, max_tokens=512)
        if output.startswith("```json"):
            output = output[7:-3].strip()
        elif output.startswith("```"):
            output = output[3:-3].strip()
        result = json.loads(output)
        std = result["standard_tag"]
        aliases = result.get("aliases", [])
        if not isinstance(aliases, list):
            aliases = []

        if std not in cluster:
            raise ValueError("standard_tag not in input")
        valid_aliases = [a for a in aliases if a in cluster and a != std]
        remaining = [t for t in cluster if t != std and t not in valid_aliases]
        valid_aliases.extend(remaining)

        result_list.append({
            "standard_tag": std,
            "aliases": valid_aliases
        })
    except Exception as e:
        print(f"  âš ï¸ ç°‡æ ‡å‡†åŒ–å¤±è´¥ï¼Œå›é€€é€‰æœ€é•¿æ ‡ç­¾: {e}")
        std = max(cluster, key=len)
        aliases = [t for t in cluster if t != std]
        result_list.append({
            "standard_tag": std,
            "aliases": aliases
        })


# ======================
# ç¬¬äºŒæ­¥ï¼šæ„å»ºåˆ†ç±»ä½“ç³»ï¼ˆè¾“å‡º [ä¸»ç±», å­ç±»] åˆ—è¡¨ï¼‰
# ======================

def build_category_system(article_metas: Dict[str, Dict]) -> List[Dict[str, Any]]:
    """
    ç›´æ¥ä» .ontology/category_schema.json åŠ è½½é¢„å®šä¹‰åˆ†ç±»ä½“ç³»ï¼Œ
    ä¸å†é€šè¿‡ LLM åŠ¨æ€ç”Ÿæˆå­ç±»ã€‚

    è¾“å‡ºæ ¼å¼ï¼š
      - ä¸å¯ç»†åˆ†ä¸»ç±» â†’ ["ä¸»ç±»"]
      - å¯ç»†åˆ†ä¸»ç±» â†’ ["ä¸»ç±»", "å­ç±»"]ï¼ˆä½¿ç”¨ schema ä¸­çš„ subcategoriesï¼‰
    """
    schema_path = Path(".ontology/category_schema.json")
    if not schema_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°åˆ†ç±»æ¶æ„æ–‡ä»¶: {schema_path}")

    with open(schema_path, "r", encoding="utf-8") as f:
        schema = json.load(f)

    result_categories = []

    for cat_def in schema["main_categories"]:
        main_name = cat_def["name"]
        description = cat_def["description"]
        allow_sub = cat_def.get("allow_subcategory", False)

        if not allow_sub:
            # å•å±‚è·¯å¾„
            result_categories.append({
                "path": [main_name],
                "description": description
            })
        else:
            # åŒå±‚è·¯å¾„ï¼šä½¿ç”¨é¢„å®šä¹‰çš„ subcategories
            subcats = cat_def.get("subcategories", [])
            if not subcats:
                # è‹¥æ— å­ç±»ï¼Œè‡³å°‘ä¿ç•™ä¸€ä¸ªå ä½ï¼ˆé¿å…ä¸»ç±»æ— å­ç±»ï¼‰
                subcats = ["é€šç”¨å®è·µ"]
            for sub in subcats:
                result_categories.append({
                    "path": [main_name, sub],
                    "description": f"{description} â€”â€” å…·ä½“æ–¹å‘ï¼š{sub}"
                })

    print(f"âœ… ä» category_schema.json åŠ è½½ {len(result_categories)} ä¸ªé¢„å®šä¹‰åˆ†ç±»")
    for idx, cat in enumerate(result_categories, 1):
        path_str = " / ".join(cat["path"])
        print(f"  {idx}. {path_str} â€”â€” {cat['description']}")

    return result_categories


# ======================
# ç¬¬ä¸‰æ­¥ï¼šåˆ†é…ï¼ˆæœ€å¤š 3 ä¸ª [ä¸»ç±», å­ç±»]ï¼‰
# ======================


def assign_categories_and_tags(
        article_metas: Dict[str, Dict],
        categories: List[Dict],
        standardized_tags: List[Dict]
) -> List[Dict]:
    """
    ä¸ºæ¯ç¯‡æ–‡ç« åˆ†é…åˆ†ç±»è·¯å¾„å’Œæ ‡å‡†åŒ–æ ‡ç­¾ã€‚

    è·¯å¾„è§„åˆ™ï¼š
      - ä¸å¯ç»†åˆ†ä¸»ç±»ï¼ˆallow_subcategory=falseï¼‰â†’ ä»…å…è®¸å•å±‚è·¯å¾„ï¼Œå¦‚ ["äººç”Ÿæ„Ÿæ‚Ÿ"]
      - å¯ç»†åˆ†ä¸»ç±»ï¼ˆallow_subcategory=trueï¼‰â†’ å¿…é¡»ä¸ºåŒå±‚è·¯å¾„ï¼Œå¦‚ ["è®ºæ–‡ç²¾è¯»", "æ‰©æ•£æ¨¡å‹"]
    """
    from pathlib import Path

    # === 1. åŠ è½½ä¸»ç±» schemaï¼ˆåŠ¨æ€è·å–è§„åˆ™ï¼‰===
    schema_path = Path(".ontology/category_schema.json")
    if not schema_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°åˆ†ç±»æ¶æ„æ–‡ä»¶: {schema_path}")

    with open(schema_path, "r", encoding="utf-8") as f:
        schema = json.load(f)

    main_defs = {cat["name"]: cat for cat in schema["main_categories"]}
    VALID_MAINS = set(main_defs.keys())
    NON_SUBDIVIDABLE = {name for name, d in main_defs.items() if not d["allow_subcategory"]}
    SUBDIVIDABLE = VALID_MAINS - NON_SUBDIVIDABLE

    # === 2. æ„å»ºæ ‡å‡†æ ‡ç­¾æ˜ å°„ï¼ˆå«åˆ«åï¼‰===
    alias_to_std = {}
    for item in standardized_tags:
        std_tag = item.get("standard_tag", "").strip()
        if not std_tag:
            continue
        alias_to_std[std_tag] = std_tag
        for alias in item.get("aliases", []):
            alias_clean = alias.strip()
            if alias_clean:
                alias_to_std[alias_clean] = std_tag

    # === 3. æ„å»ºä¸¥æ ¼åˆæ³•çš„è·¯å¾„é›†åˆï¼ˆtuple å½¢å¼ä¾¿äºæŸ¥æ‰¾ï¼‰===
    valid_paths = set()
    for cat in categories:
        path = cat.get("path")
        if not isinstance(path, list) or not (1 <= len(path) <= 2):
            continue
        main = path[0]
        if main not in VALID_MAINS:
            continue
        # å¼ºåˆ¶å±‚çº§åˆè§„
        if main in SUBDIVIDABLE and len(path) != 2:
            continue  # å¯ç»†åˆ†ä¸»ç±»å¿…é¡»åŒå±‚
        if main in NON_SUBDIVIDABLE and len(path) != 1:
            continue  # ä¸å¯ç»†åˆ†ä¸»ç±»å¿…é¡»å•å±‚
        valid_paths.add(tuple(path))

    if not valid_paths:
        print("âš ï¸ æ— æœ‰æ•ˆåˆ†ç±»è·¯å¾„ï¼Œè·³è¿‡åˆ†é…")
        return [{"file": f"{stem}.md", "categories": [], "tags": []} for stem in article_metas]

    # === 4. å¼€å§‹åˆ†é… ===
    total = len(article_metas)
    print(f"ğŸ¯ å¼€å§‹åˆ†é…åˆ†ç±»ä¸æ ‡ç­¾ï¼ˆå…± {total} ç¯‡æ–‡ç« ï¼‰...")

    ASSIGN_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„åšå®¢åˆ†ç±»ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ–‡ç« ä»**ä¸‹æ–¹æ˜ç¡®åˆ—å‡ºçš„è·¯å¾„ä¸­**é€‰æ‹©æœ€ç›¸å…³çš„åˆ†ç±»ã€‚

ğŸ“Œ è¦æ±‚ï¼š
- æœ€å¤šé€‰æ‹© 3 ä¸ªåˆ†ç±»è·¯å¾„ï¼›
- **å¿…é¡»ä¸¥æ ¼ä½¿ç”¨â€œå¯ç”¨åˆ†ç±»è·¯å¾„â€ä¸­çš„æ¡ç›®**ï¼›
- è·¯å¾„æ ¼å¼è¯´æ˜ï¼š
  â€¢ å•å±‚ï¼š["äººç”Ÿæ„Ÿæ‚Ÿ"] â€”â€” ä»…ç”¨äºä¸å¯ç»†åˆ†ä¸»ç±»
  â€¢ åŒå±‚ï¼š["è¸©å‘è®°å½•", "CUDAé…ç½®"] â€”â€” ç”¨äºå…¶ä½™æ‰€æœ‰ä¸»ç±»
- å¦‚æœæ²¡æœ‰ç›¸å…³é¡¹ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚

å¯ç”¨åˆ†ç±»è·¯å¾„ï¼ˆå…± {n} æ¡ï¼‰ï¼š
{cat_options}

æ–‡ç« ä¿¡æ¯ï¼š
- å†™ä½œç±»å‹: {type_hint}
- ä¸»æ—¨: {idea}
- ç›¸å…³æ ‡ç­¾: {tags_str}

è¾“å‡ºæ ¼å¼ï¼ˆä»… JSONï¼‰ï¼š
{{
  "categories": [
    ["ä¸»ç±»"],
    ["ä¸»ç±»", "å­ç±»"],
    ...
  ]
}}
"""

    assignments = []
    failed = 0

    # æ’åºè·¯å¾„ä»¥ä¾¿å±•ç¤ºæ›´æ¸…æ™°ï¼ˆå…ˆæŒ‰ä¸»ç±»ï¼Œå†æŒ‰æ˜¯å¦åŒå±‚ï¼‰
    display_paths = sorted(valid_paths, key=lambda p: (p[0], len(p), p[1] if len(p) > 1 else ""))
    cat_options = "\n".join(f"- {list(p)}" for p in display_paths)

    for idx, (stem, meta) in enumerate(article_metas.items(), 1):
        print(f"[{idx}/{total}] ğŸ“„ åˆ†é…: {stem}")

        # --- æ ‡å‡†åŒ–æ ‡ç­¾ ---
        raw_tags = meta.get("tags", [])
        final_tags = []
        for rt in raw_tags:
            rt_clean = str(rt).strip()
            if rt_clean:
                final_tags.append(alias_to_std.get(rt_clean, rt_clean))

        # --- å‡†å¤‡ LLM è¾“å…¥ ---
        idea = meta.get("main_idea", "").strip() or "ï¼ˆæ— ä¸»æ—¨ï¼‰"
        type_hint = meta.get("content_type", "").strip() or "ï¼ˆæœªçŸ¥ç±»å‹ï¼‰"
        tags_str = ", ".join(final_tags[:10]) if final_tags else "ï¼ˆæ— æ ‡ç­¾ï¼‰"

        final_cats = []
        try:
            prompt = ASSIGN_PROMPT.format(
                n=len(display_paths),
                cat_options=cat_options,
                type_hint=type_hint,
                idea=idea,
                tags_str=tags_str
            )
            output = call_llm(prompt, temperature=0.0, max_tokens=512)

            # æ¸…ç†å¯èƒ½çš„ markdown åŒ…è£¹
            if output.startswith("```json"):
                output = output[7:-3].strip()
            elif output.startswith("```"):
                output = output[3:-3].strip()

            data = json.loads(output)
            candidate_cats = data.get("categories", [])

            if isinstance(candidate_cats, list):
                for c in candidate_cats:
                    if isinstance(c, list) and tuple(c) in valid_paths:
                        final_cats.append(c)
                        if len(final_cats) >= 3:
                            break
        except Exception as e:
            failed += 1
            print(f"  âš ï¸ åˆ†é…å¤±è´¥ ({stem}): {type(e).__name__}: {e}")
            final_cats = []

        assignments.append({
            "file": f"{stem}.md",
            "categories": final_cats,
            "tags": final_tags
        })

    print(f"âœ… åˆ†ç±»ä¸æ ‡ç­¾åˆ†é…å®Œæˆï¼å…± {total} ç¯‡ï¼Œå¤±è´¥ {failed} ç¯‡")
    return assignments


# ======================
# ä¸»å‡½æ•°
# ======================

def main():
    print("ğŸ“¥ Stage 2: æ„å»ºç»Ÿä¸€ Ontologyï¼ˆæ”¯æŒæœ€å¤š 3 ä¸ª [ä¸»ç±», å­ç±»] ç±»åˆ«ï¼‰")

    metas = load_all_meta()
    if not metas:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆå…ƒæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ stage1")
        return

    # Step 1: æ ‡å‡†åŒ–æ ‡ç­¾ï¼ˆå…¨å±€ä¸€æ¬¡ï¼Œæ”¯æŒç¼“å­˜ï¼‰
    print("\nğŸ·ï¸ æ­¥éª¤ 1: æ ‡å‡†åŒ–æ ‡ç­¾...")

    tags_cache_path = OUTPUT_DIR / "tags.json"

    if tags_cache_path.exists():
        print(f"ğŸ“‚ æ£€æµ‹åˆ°æ ‡ç­¾ç¼“å­˜æ–‡ä»¶: {tags_cache_path}")
        try:
            with open(tags_cache_path, "r", encoding="utf-8") as f:
                cache_data = json.load(f)
            standardized_tags = cache_data.get("tags", [])
            if standardized_tags:
                print(f"âœ… æˆåŠŸåŠ è½½ {len(standardized_tags)} ä¸ªæ ‡å‡†åŒ–æ ‡ç­¾ï¼ˆè·³è¿‡ LLM èšç±»ï¼‰")
            else:
                print("âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸­æ— æœ‰æ•ˆæ ‡ç­¾ï¼Œå°†é‡æ–°ç”Ÿæˆ...")
                standardized_tags = None
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥ ({e})ï¼Œå°†é‡æ–°ç”Ÿæˆæ ‡ç­¾...")
            standardized_tags = None
    else:
        standardized_tags = None

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç¼“å­˜ï¼Œåˆ™æ‰§è¡Œæ ‡å‡†åŒ–
    if standardized_tags is None:
        all_raw_tags = [tag for m in metas.values() for tag in m.get("tags", [])]
        standardized_tags = normalize_tags(all_raw_tags)
        # ä¿å­˜ç¼“å­˜
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        with open(tags_cache_path, "w", encoding="utf-8") as f:
            json.dump({"tags": standardized_tags}, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ ‡å‡†åŒ–æ ‡ç­¾å·²ä¿å­˜è‡³: {tags_cache_path}")

    # Step 2: æ„å»ºåˆ†ç±»ä½“ç³»ï¼ˆç›´æ¥åŠ è½½é¢„å®šä¹‰ schemaï¼Œæ— éœ€ç¼“å­˜æˆ– LLMï¼‰
    print("\nğŸ—‚ï¸ æ­¥éª¤ 2: åŠ è½½é¢„å®šä¹‰åˆ†ç±»ä½“ç³»ï¼ˆæ¥è‡ª category_schema.jsonï¼‰...")

    categories = build_category_system(metas)
    categories_cache_path = OUTPUT_DIR / "categories.json"
    with open(categories_cache_path, "w", encoding="utf-8") as f:
        json.dump({"categories": categories}, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ åˆ†ç±»ä½“ç³»å·²ä¿å­˜è‡³: {categories_cache_path}")

    # Step 3: åˆ†é…ï¼ˆä½¿ç”¨å…¨å±€æ˜ å°„ï¼‰
    print("\nğŸ¯ æ­¥éª¤ 3: åˆ†é…ç±»åˆ«ä¸æ ‡ç­¾ï¼ˆæ¯ç¯‡æœ€å¤š 3 ä¸ª [ä¸»ç±», å­ç±»]ï¼‰...")
    assignments = assign_categories_and_tags(metas, categories, standardized_tags)
    with open(OUTPUT_DIR / "assignment.json", "w", encoding="utf-8") as f:
        json.dump(assignments, f, ensure_ascii=False, indent=2)

    # Summary
    print("\nğŸ‰ Stage 2 å®Œæˆï¼")
    print(f"  æ ‡å‡†åŒ–æ ‡ç­¾æ•°: {len(standardized_tags)}")
    print(f"  å…¨å±€åˆ†ç±»æ•°: {len(categories)}")
    print(f"  æ–‡ç« åˆ†é…æ•°: {len(assignments)}")
    print(f"\nğŸ“Š LLM ç»Ÿè®¡:")
    print(f"  è°ƒç”¨æ¬¡æ•°: {STATS['total_calls']}")
    print(f"  æ€» tokens: {STATS['total_prompt_tokens'] + STATS['total_completion_tokens']}")


if __name__ == "__main__":
    main()
